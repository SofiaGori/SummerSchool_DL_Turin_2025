{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f002981f-36e3-4a78-b566-7ec9d3296673",
   "metadata": {
    "id": "f002981f-36e3-4a78-b566-7ec9d3296673"
   },
   "source": [
    "### DeepHAM to solve KS model: policy function optimization with blanks (code on Nuvolos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rutV-PKrEIIh",
   "metadata": {
    "id": "rutV-PKrEIIh"
   },
   "source": [
    "## Instructions\n",
    "\n",
    "> In DeepHAM, the policy function is trained with a dedicated class.\n",
    "> You will now **implement this class directly** in the notebook instead of importing it.\n",
    ">\n",
    "> We give you the skeleton (`PolicyTrainer` and `KSPolicyTrainer`) with some **TODO blanks**:\n",
    ">\n",
    "> * Write the **objective** (`loss`) for Krusell–Smith.\n",
    "> * Use **`stop_gradient`** for fictitious play (game case).\n",
    "> * Implement the **household dynamics simulation (use budget constraint)** and update of wealth/capital.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VWEsIHDruTO3",
   "metadata": {
    "id": "VWEsIHDruTO3"
   },
   "source": [
    "## Change to the code directory on Nuvolos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NTQoc_SZuSqH",
   "metadata": {
    "id": "NTQoc_SZuSqH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/files/day2/Yang/code/DeepHAM_nuvolos/src')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9LsHjQ45YLCu",
   "metadata": {
    "id": "9LsHjQ45YLCu"
   },
   "source": [
    "#### code on local machine starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "Vl3xZ0eM3IkG",
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1756172589121,
     "user": {
      "displayName": "Yucheng Yang",
      "userId": "06663760817085207108"
     },
     "user_tz": -120
    },
    "id": "Vl3xZ0eM3IkG"
   },
   "outputs": [],
   "source": [
    "# Define the configurations directly instead of using absl flags\n",
    "config_path = \"./configs/KS/game_nn_n50_0fm1gm_test.json\"\n",
    "exp_name = \"1gm_test\"\n",
    "seed_index = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8Y2ArTNiBtWW",
   "metadata": {
    "id": "8Y2ArTNiBtWW"
   },
   "source": [
    "### Import everything except the policy class to be written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tdF6Fll-30TR",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1756172593942,
     "user": {
      "displayName": "Yucheng Yang",
      "userId": "06663760817085207108"
     },
     "user_tz": -120
    },
    "id": "tdF6Fll-30TR"
   },
   "outputs": [],
   "source": [
    "# Imports from the original script\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from param import KSParam\n",
    "from dataset import KSInitDataSet\n",
    "from value import ValueTrainer\n",
    "from util import print_elapsedtime\n",
    "from util import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "zgHaVGJvrg3J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 844,
     "status": "ok",
     "timestamp": 1756172596294,
     "user": {
      "displayName": "Yucheng Yang",
      "userId": "06663760817085207108"
     },
     "user_tz": -120
    },
    "id": "zgHaVGJvrg3J",
    "outputId": "8b6d577c-2a5f-4491-c5b2-c9410ea03a42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using seed 789 (index 3)\n",
      "Solving the problem based on the config path ./configs/KS/game_nn_n50_0fm1gm_test.json\n"
     ]
    }
   ],
   "source": [
    "# Load the configuration from the JSON file\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "if \"random_seed\" in config:\n",
    "    seed = config[\"random_seed\"][seed_index]\n",
    "    set_random_seed(seed)\n",
    "    print(f\"Using seed {seed} (index {seed_index})\")\n",
    "\n",
    "print(\"Solving the problem based on the config path {}\".format(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6Uqvcw7d32YH",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1756172599548,
     "user": {
      "displayName": "Yucheng Yang",
      "userId": "06663760817085207108"
     },
     "user_tz": -120
    },
    "id": "6Uqvcw7d32YH"
   },
   "outputs": [],
   "source": [
    "mparam = KSParam(config[\"n_agt\"], config[\"beta\"], config[\"mats_path\"])\n",
    "# save config at the beginning for checking\n",
    "model_path = \"../data/simul_results/KS/{}_{}_n{}_{}\".format(\n",
    "    \"game\" if config[\"policy_config\"][\"opt_type\"] == \"game\" else \"sp\",\n",
    "    config[\"dataset_config\"][\"value_sampling\"],\n",
    "    config[\"n_agt\"],\n",
    "    exp_name,\n",
    ")\n",
    "config[\"model_path\"] = model_path\n",
    "config[\"current_time\"] = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "with open(os.path.join(model_path, \"config_beg.json\"), 'w') as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LZlS_jbV2rhn",
   "metadata": {
    "id": "LZlS_jbV2rhn"
   },
   "source": [
    "## Step 1 — Dataset & policy initialization (one-time setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8L236Ba2ht5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50881,
     "status": "ok",
     "timestamp": 1756172653051,
     "user": {
      "displayName": "Yucheng Yang",
      "userId": "06663760817085207108"
     },
     "user_tz": -120
    },
    "id": "b8L236Ba2ht5",
    "outputId": "1692a47b-a1ad-4f87-81bd-1f24c9532076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of total utility 20.068157.\n",
      "The dataset has 4608 samples in total.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup & dataset ---\n",
    "start_time   = time.monotonic()\n",
    "init_ds      = KSInitDataSet(mparam, config)\n",
    "value_config = config[\"value_config\"]\n",
    "\n",
    "# --- Initial policy choice used to build the first value-training dataset ---\n",
    "if config[\"init_with_bchmk\"]:\n",
    "    init_policy = init_ds.k_policy_bchmk     # PDE / bspline benchmark policy\n",
    "    policy_type = \"pde\"\n",
    "else:\n",
    "    init_policy = init_ds.c_policy_const_share  # constant consumption share NN policy\n",
    "    policy_type = \"nn_share\"\n",
    "\n",
    "# --- Build value-training datasets from the chosen initial policy (supervised targets) ---\n",
    "train_vds, valid_vds = init_ds.get_valuedataset(\n",
    "    init_policy,\n",
    "    policy_type,\n",
    "    update_init=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xfm1boI6267z",
   "metadata": {
    "id": "Xfm1boI6267z"
   },
   "source": [
    "## Step 2 — Initial value-function training (before any policy optimization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "_xnf5tdG25DV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29378,
     "status": "ok",
     "timestamp": 1756172682431,
     "user": {
      "displayName": "Yucheng Yang",
      "userId": "06663760817085207108"
     },
     "user_tz": -120
    },
    "id": "_xnf5tdG25DV",
    "outputId": "39d2b200-d48d-425c-af2f-80eb3f8e9b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function learning epoch: 0\n",
      "Value function learning epoch: 20\n",
      "Value function learning epoch: 40\n",
      "Value function learning epoch: 0\n",
      "Value function learning epoch: 20\n",
      "Value function learning epoch: 40\n",
      "Value function learning epoch: 0\n",
      "Value function learning epoch: 20\n",
      "Value function learning epoch: 40\n",
      "CPU times: user 30.5 s, sys: 2.12 s, total: 32.6 s\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vtrainers = []\n",
    "for i in range(value_config[\"num_vnet\"]):\n",
    "    config[\"vnet_idx\"] = str(i)\n",
    "    vtrainers.append(ValueTrainer(config))\n",
    "\n",
    "for vtr in vtrainers:\n",
    "    vtr.train(train_vds, valid_vds, value_config[\"num_epoch\"], value_config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417DaJ_s3BRo",
   "metadata": {
    "id": "417DaJ_s3BRo"
   },
   "source": [
    "> **Notes for readers:**\n",
    ">\n",
    "> * We pre-train the value network(s) once on data generated by an initial policy.\n",
    "> * Each `ValueTrainer.train(...)` runs for `value_config[\"num_epoch\"]` epochs over the prepared datasets.\n",
    "> * These $V$ nets will be used as the terminal bootstrap $\\beta^T V(s_T)$ inside policy optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FV6j5Tzb3PL7",
   "metadata": {
    "id": "FV6j5Tzb3PL7"
   },
   "source": [
    "\n",
    "## Step 3 — Policy training with periodic value function updating\n",
    "\n",
    "**What happens here.**\n",
    "\n",
    "* We launch `KSPolicyTrainer.train(num_step, batch_size)`.\n",
    "* **Every step**:\n",
    "\n",
    "  * draw a **fresh mini-batch** from `policy_ds` and **simulate new shocks** (`sampler`),\n",
    "  * take **one policy gradient step** (`train_step`).\n",
    "* **Policy-dataset refresh (from simulation)**:\n",
    "\n",
    "  * Inside `sampler`, when `policy_ds.epoch_used > epoch_resample`, we **rebuild the dataset** via `update_policydataset(update_init)`.\n",
    "  * With your config `epoch_resample = 0`, this means: **rebuild after each full pass over the dataset** (cadence ≈ `ceil(dataset_rows / batch_size)` steps).\n",
    "  * If `update_init=True` (set right after value retraining), the next rebuild is a **hard refresh**: we also update dataset stats from the new simulation.\n",
    "* **Every `freq_valid` steps**: run validation on a fixed validation set.\n",
    "* **Every `freq_update_v` steps** (if `value_sampling != \"bchmk\"`):\n",
    "\n",
    "  * **rebuild value datasets** under the **current policy**,\n",
    "  * **retrain** each value net for `value_config[\"num_epoch\"]` epochs,\n",
    "  * set `update_init=True` so the **next policy-dataset rebuild** performs a **hard refresh**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fDPAlwctCC33",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1756173087056,
     "user": {
      "displayName": "Yucheng Yang",
      "userId": "06663760817085207108"
     },
     "user_tz": -120
    },
    "id": "fDPAlwctCC33"
   },
   "outputs": [],
   "source": [
    "# Iterative policy and value training\n",
    "policy_config = config[\"policy_config\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rDlWTMjIEsci",
   "metadata": {
    "id": "rDlWTMjIEsci"
   },
   "source": [
    "\n",
    "## Block A — PolicyTrainer (base class)\n",
    "\n",
    "> This is the **generic policy trainer** (abstract base class). It handles:\n",
    ">\n",
    "> * State preparation (`prepare_state`),\n",
    "> * Policy evaluation (`policy_fn`),\n",
    "> * Gradient calculation (`grad`),\n",
    "> * Training loop (`train`).\n",
    ">\n",
    "> You **do not need to change this**. Read through it carefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "artzNQQwCCme",
   "metadata": {
    "executionInfo": {
     "elapsed": 1571,
     "status": "ok",
     "timestamp": 1756173208708,
     "user": {
      "displayName": "Yucheng Yang",
      "userId": "06663760817085207108"
     },
     "user_tz": -120
    },
    "id": "artzNQQwCCme"
   },
   "outputs": [],
   "source": [
    "from policy import PolicyTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oYDlN_KHFXe6",
   "metadata": {
    "id": "oYDlN_KHFXe6"
   },
   "source": [
    "## Block B — KSPolicyTrainer (student exercise)\n",
    "\n",
    "> Now implement the **objective**.\n",
    ">\n",
    "> Fill in the blanks:\n",
    ">\n",
    "> 1. **Policy output:** use `self.policy_fn(full_state_dict)[...,0]`.\n",
    "> 2. **Game case:** apply `tf.stop_gradient` to fix others.\n",
    "> 3. **Factor prices:** compute `R` and `wage` with the KS formulas.\n",
    "> 4. **Budget constraint:** update `wealth`, `csmp`, and next-period `k_cross`.\n",
    "> 5. **Utility accumulation:** add `β^t log(csmp)`.\n",
    "> 6. **Terminal bootstrap:** average value net predictions at `t_unroll-1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9_1KwqFcFJDt",
   "metadata": {
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1756173248460,
     "user": {
      "displayName": "Yucheng Yang",
      "userId": "06663760817085207108"
     },
     "user_tz": -120
    },
    "id": "9_1KwqFcFJDt"
   },
   "outputs": [],
   "source": [
    "class KSPolicyTrainer(PolicyTrainer):\n",
    "    def __init__(self, vtrainers, init_ds, policy_path=None):\n",
    "        super().__init__(vtrainers, init_ds, policy_path)\n",
    "        if self.config[\"init_with_bchmk\"]:\n",
    "            init_policy = self.init_ds.k_policy_bchmk\n",
    "            policy_type = \"pde\"\n",
    "        else:\n",
    "            init_policy = self.init_ds.c_policy_const_share\n",
    "            policy_type = \"nn_share\"\n",
    "        self.policy_ds = self.init_ds.get_policydataset(init_policy, policy_type, update_init=False)\n",
    "\n",
    "    @tf.function\n",
    "    def loss(self, input_data):\n",
    "        k_cross = input_data[\"k_cross\"]\n",
    "        ashock, ishock = input_data[\"ashock\"], input_data[\"ishock\"]\n",
    "        util_sum = 0\n",
    "\n",
    "        for t in range(self.t_unroll):\n",
    "            k_mean = tf.reduce_mean(k_cross, axis=1, keepdims=True)\n",
    "            k_mean_tmp = tf.tile(k_mean, [1, self.mparam.n_agt])\n",
    "            k_mean_tmp = tf.expand_dims(k_mean_tmp, axis=-1)\n",
    "            i_tmp = ishock[:, :, t:t+1]\n",
    "            a_tmp = tf.tile(ashock[:, t:t+1], [1, self.mparam.n_agt])\n",
    "            a_tmp = tf.expand_dims(a_tmp, axis=2)\n",
    "\n",
    "            basic_s_tmp = tf.concat(\n",
    "                [tf.expand_dims(k_cross, axis=-1), k_mean_tmp, a_tmp, i_tmp],\n",
    "                axis=-1\n",
    "            )\n",
    "            basic_s_tmp = self.init_ds.normalize_data(basic_s_tmp, key=\"basic_s\", withtf=True)\n",
    "            full_state_dict = {\n",
    "                \"basic_s\": basic_s_tmp,\n",
    "                \"agt_s\": self.init_ds.normalize_data(tf.expand_dims(k_cross, axis=-1), key=\"agt_s\", withtf=True)\n",
    "            }\n",
    "\n",
    "            if t == self.t_unroll - 1:\n",
    "                # --- terminal bootstrap ---\n",
    "                value = 0\n",
    "                for vtr in self.vtrainers:\n",
    "                    value += self.init_ds.unnormalize_data(\n",
    "                        vtr.value_fn(full_state_dict)[..., 0], key=\"value\", withtf=True\n",
    "                    )\n",
    "                value /= self.num_vnet\n",
    "                util_sum += self.discount[t]*value\n",
    "                continue\n",
    "\n",
    "            # (1) policy output\n",
    "            c_share = ...   # TODO: call self.policy_fn(full_state_dict)[...,0]\n",
    "\n",
    "            # (2) game case\n",
    "            if self.policy_config[\"opt_type\"] == \"game\":\n",
    "                c_share = tf.concat(\n",
    "                    [c_share[:, 0:1], tf.stop_gradient(c_share[:, 1:])],\n",
    "                    axis=1\n",
    "                )\n",
    "\n",
    "            # (3) prices\n",
    "            tau = tf.where(ashock[:, t:t+1] < 1, self.mparam.tau_b, self.mparam.tau_g)\n",
    "            emp = tf.where(\n",
    "                ashock[:, t:t+1] < 1,\n",
    "                self.mparam.l_bar*self.mparam.er_b,\n",
    "                self.mparam.l_bar*self.mparam.er_g\n",
    "            )\n",
    "            tau, emp = tf.cast(tau, DTYPE), tf.cast(emp, DTYPE)\n",
    "            R    = ...   # TODO: rental rate formula\n",
    "            wage = ...   # TODO: wage formula\n",
    "\n",
    "            # (4) budget\n",
    "            wealth = ...  # TODO: R*k_cross + (1-tau)*wage*l_bar*ishock + mu*wage*(1-ishock)\n",
    "            csmp   = tf.clip_by_value(c_share * wealth, EPSILON, wealth-EPSILON)\n",
    "            k_cross = wealth - csmp\n",
    "\n",
    "            # (5) utility\n",
    "            util_sum += self.discount[t] * tf.math.log(csmp)\n",
    "\n",
    "        if self.policy_config[\"opt_type\"] == \"socialplanner\":\n",
    "            output_dict = {\"m_util\": -tf.reduce_mean(util_sum), \"k_end\": tf.reduce_mean(k_cross)}\n",
    "        elif self.policy_config[\"opt_type\"] == \"game\":\n",
    "            output_dict = {\"m_util\": -tf.reduce_mean(util_sum[:, 0]), \"k_end\": tf.reduce_mean(k_cross)}\n",
    "        return output_dict\n",
    "\n",
    "    def update_policydataset(self, update_init=False):\n",
    "        self.policy_ds = self.init_ds.get_policydataset(self.current_c_policy, \"nn_share\", update_init)\n",
    "\n",
    "    def get_valuedataset(self, update_init=False):\n",
    "        return self.init_ds.get_valuedataset(self.current_c_policy, \"nn_share\", update_init)\n",
    "\n",
    "    def current_c_policy(self, k_cross, ashock, ishock):\n",
    "        k_mean = np.mean(k_cross, axis=1, keepdims=True)\n",
    "        k_mean = np.repeat(k_mean, self.mparam.n_agt, axis=1)\n",
    "        ashock = np.repeat(ashock, self.mparam.n_agt, axis=1)\n",
    "        basic_s = np.stack([k_cross, k_mean, ashock, ishock], axis=-1)\n",
    "        basic_s = self.init_ds.normalize_data(basic_s, key=\"basic_s\")\n",
    "        basic_s = basic_s.astype(NP_DTYPE)\n",
    "        full_state_dict = {\n",
    "            \"basic_s\": basic_s,\n",
    "            \"agt_s\": self.init_ds.normalize_data(k_cross[:, :, None], key=\"agt_s\")\n",
    "        }\n",
    "        c_share = self.policy_fn(full_state_dict)[..., 0]\n",
    "        return c_share\n",
    "\n",
    "    def simul_shocks(self, n_sample, T, mparam, state_init):\n",
    "        return KS.simul_shocks(n_sample, T, mparam, state_init)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arfxNJcOGjjl",
   "metadata": {
    "id": "arfxNJcOGjjl"
   },
   "source": [
    "## Block C — Training call\n",
    "\n",
    "> Once you fill the blanks, you can run the training just like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QBG7i6r4NCdl",
   "metadata": {
    "id": "QBG7i6r4NCdl"
   },
   "outputs": [],
   "source": [
    "ptrainer = KSPolicyTrainer(vtrainers, init_ds)\n",
    "ptrainer.train(policy_config[\"num_step\"], policy_config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z9JmD2634U19",
   "metadata": {
    "id": "Z9JmD2634U19"
   },
   "source": [
    "\n",
    "> **Notes for readers:**\n",
    ">\n",
    "> * **Mini-batch & shocks:** new **every step**.\n",
    "> * **Policy-dataset cadence:** with `t_sample=200` and `t_skip=4`, each path contributes \\~50 time-slices;\n",
    ">   dataset rows ≈ `n_path * 50` (minus NaN rows). Rebuild after each full pass:\n",
    ">   `steps_per_dataset ≈ ceil(dataset_rows / batch_size)`.\n",
    ">   Example: if `n_path=384`, rows ≈ `384*50=19,200` → `19,200/384=50` steps per rebuild.\n",
    "> * **Validation:** every `freq_valid=500` steps (20 times for `num_step=10,000`).\n",
    "> * **Value retrain:** every `freq_update_v=2000` steps (5 times total). This sets `update_init=True`; the **next** dataset rebuild then also updates dataset statistics from the new simulation (hard refresh).\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IufSrjPnl775",
   "metadata": {
    "id": "IufSrjPnl775"
   },
   "outputs": [],
   "source": [
    "# Save config and models\n",
    "with open(os.path.join(model_path, \"config.json\"), 'w') as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "for i, vtr in enumerate(vtrainers):\n",
    "    vtr.save_model(os.path.join(model_path, \"value{}.weights.h5\".format(i)))\n",
    "\n",
    "ptrainer.save_model(os.path.join(model_path, \"policy.weights.h5\"))\n",
    "\n",
    "end_time = time.monotonic()\n",
    "print_elapsedtime(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6OJ12AF432c2",
   "metadata": {
    "id": "6OJ12AF432c2"
   },
   "outputs": [],
   "source": [
    "model_path"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
