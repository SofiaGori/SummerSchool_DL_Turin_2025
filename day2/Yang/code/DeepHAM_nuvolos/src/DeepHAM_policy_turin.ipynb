{"cells":[{"cell_type":"markdown","id":"f002981f-36e3-4a78-b566-7ec9d3296673","metadata":{"id":"f002981f-36e3-4a78-b566-7ec9d3296673"},"source":["### DeepHAM to solve KS model: policy function optimization with blanks (code on Nuvolos)"]},{"cell_type":"markdown","source":["## Instructions\n","\n","> In DeepHAM, the policy function is trained with a dedicated class.\n","> You will now **implement this class directly** in the notebook instead of importing it.\n",">\n","> We give you the skeleton (`PolicyTrainer` and `KSPolicyTrainer`) with some **TODO blanks**:\n",">\n","> * Write the **objective** (`loss`) for Krusell–Smith.\n","> * Use **`stop_gradient`** for fictitious play (game case).\n","> * Implement the **household dynamics simulation (use budget constraint)** and update of wealth/capital.\n"],"metadata":{"id":"rutV-PKrEIIh"},"id":"rutV-PKrEIIh"},{"cell_type":"code","execution_count":1,"id":"u0vokfV4I817","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1756172081491,"user":{"displayName":"Yucheng Yang","userId":"06663760817085207108"},"user_tz":-120},"id":"u0vokfV4I817","outputId":"7e8e1213-336f-41b4-fd79-3ac538e9bd24"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Aug 26 01:34:41 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0             50W /  400W |       0MiB /  40960MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!/opt/bin/nvidia-smi"]},{"cell_type":"code","execution_count":2,"id":"xd9cqiJlI8_u","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24920,"status":"ok","timestamp":1756172106414,"user":{"displayName":"Yucheng Yang","userId":"06663760817085207108"},"user_tz":-120},"id":"xd9cqiJlI8_u","outputId":"2aae7a7a-32f0-4a30-edef-992f88858f5a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"id":"FceY6j28I9DM","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":2303,"status":"ok","timestamp":1756172108718,"user":{"displayName":"Yucheng Yang","userId":"06663760817085207108"},"user_tz":-120},"id":"FceY6j28I9DM","outputId":"4aaa51ec-663b-488b-ba86-6dd21ff0327a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/DeepHAM_Turin/src'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["import os\n","os.chdir('/content/drive/MyDrive/DeepHAM_Turin/src')\n","os.getcwd()"]},{"cell_type":"code","execution_count":4,"id":"2nOxkcVa3_Yr","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8530,"status":"ok","timestamp":1756172117254,"user":{"displayName":"Yucheng Yang","userId":"06663760817085207108"},"user_tz":-120},"id":"2nOxkcVa3_Yr","outputId":"9f33de0d-da3b-4516-b294-45318a4f69c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting quantecon\n","  Downloading quantecon-0.9.0-py3-none-any.whl.metadata (5.3 kB)\n","Requirement already satisfied: numba>=0.49.0 in /usr/local/lib/python3.12/dist-packages (from quantecon) (0.60.0)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from quantecon) (2.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from quantecon) (2.32.4)\n","Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from quantecon) (1.16.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from quantecon) (1.13.3)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.49.0->quantecon) (0.43.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->quantecon) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->quantecon) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->quantecon) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->quantecon) (2025.8.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->quantecon) (1.3.0)\n","Downloading quantecon-0.9.0-py3-none-any.whl (324 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/324.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: quantecon\n","Successfully installed quantecon-0.9.0\n"]}],"source":["!pip install quantecon"]},{"cell_type":"markdown","source":["## Change to the code directory on Nuvolos"],"metadata":{"id":"VWEsIHDruTO3"},"id":"VWEsIHDruTO3"},{"cell_type":"code","source":["# import os\n","# os.chdir('/files/day2/Yang/code/DeepHAM_nuvolos/src')\n","# os.getcwd()"],"metadata":{"id":"NTQoc_SZuSqH"},"id":"NTQoc_SZuSqH","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"9LsHjQ45YLCu","metadata":{"id":"9LsHjQ45YLCu"},"source":["#### code on local machine starts here"]},{"cell_type":"code","execution_count":11,"id":"Vl3xZ0eM3IkG","metadata":{"id":"Vl3xZ0eM3IkG","executionInfo":{"status":"ok","timestamp":1756172589121,"user_tz":-120,"elapsed":32,"user":{"displayName":"Yucheng Yang","userId":"06663760817085207108"}}},"outputs":[],"source":["# Define the configurations directly instead of using absl flags\n","config_path = \"./configs/KS/game_nn_n50_0fm1gm_test.json\"\n","exp_name = \"1gm_test\"\n","seed_index = 3"]},{"cell_type":"markdown","source":["### Import everything except the policy class to be written"],"metadata":{"id":"8Y2ArTNiBtWW"},"id":"8Y2ArTNiBtWW"},{"cell_type":"code","execution_count":12,"id":"tdF6Fll-30TR","metadata":{"id":"tdF6Fll-30TR","executionInfo":{"status":"ok","timestamp":1756172593942,"user_tz":-120,"elapsed":13,"user":{"displayName":"Yucheng Yang","userId":"06663760817085207108"}}},"outputs":[],"source":["# Imports from the original script\n","import json\n","import time\n","import datetime\n","from param import KSParam\n","from dataset import KSInitDataSet\n","from value import ValueTrainer\n","from util import print_elapsedtime\n","from util import set_random_seed"]},{"cell_type":"code","execution_count":13,"id":"zgHaVGJvrg3J","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":844,"status":"ok","timestamp":1756172596294,"user":{"displayName":"Yucheng Yang","userId":"06663760817085207108"},"user_tz":-120},"id":"zgHaVGJvrg3J","outputId":"8b6d577c-2a5f-4491-c5b2-c9410ea03a42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using seed 789 (index 3)\n","Solving the problem based on the config path ./configs/KS/game_nn_n50_0fm1gm_test.json\n"]}],"source":["# Load the configuration from the JSON file\n","with open(config_path, 'r') as f:\n","    config = json.load(f)\n","\n","if \"random_seed\" in config:\n","    seed = config[\"random_seed\"][seed_index]\n","    set_random_seed(seed)\n","    print(f\"Using seed {seed} (index {seed_index})\")\n","\n","print(\"Solving the problem based on the config path {}\".format(config_path))"]},{"cell_type":"code","execution_count":14,"id":"6Uqvcw7d32YH","metadata":{"id":"6Uqvcw7d32YH","executionInfo":{"status":"ok","timestamp":1756172599548,"user_tz":-120,"elapsed":16,"user":{"displayName":"Yucheng Yang","userId":"06663760817085207108"}}},"outputs":[],"source":["mparam = KSParam(config[\"n_agt\"], config[\"beta\"], config[\"mats_path\"])\n","# save config at the beginning for checking\n","model_path = \"../data/simul_results/KS/{}_{}_n{}_{}\".format(\n","    \"game\" if config[\"policy_config\"][\"opt_type\"] == \"game\" else \"sp\",\n","    config[\"dataset_config\"][\"value_sampling\"],\n","    config[\"n_agt\"],\n","    exp_name,\n",")\n","config[\"model_path\"] = model_path\n","config[\"current_time\"] = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","os.makedirs(model_path, exist_ok=True)\n","with open(os.path.join(model_path, \"config_beg.json\"), 'w') as f:\n","    json.dump(config, f)"]},{"cell_type":"markdown","source":["## Step 1 — Dataset & policy initialization (one-time setup)"],"metadata":{"id":"LZlS_jbV2rhn"},"id":"LZlS_jbV2rhn"},{"cell_type":"code","source":["# --- Setup & dataset ---\n","start_time   = time.monotonic()\n","init_ds      = KSInitDataSet(mparam, config)\n","value_config = config[\"value_config\"]\n","\n","# --- Initial policy choice used to build the first value-training dataset ---\n","if config[\"init_with_bchmk\"]:\n","    init_policy = init_ds.k_policy_bchmk     # PDE / bspline benchmark policy\n","    policy_type = \"pde\"\n","else:\n","    init_policy = init_ds.c_policy_const_share  # constant consumption share NN policy\n","    policy_type = \"nn_share\"\n","\n","# --- Build value-training datasets from the chosen initial policy (supervised targets) ---\n","train_vds, valid_vds = init_ds.get_valuedataset(\n","    init_policy,\n","    policy_type,\n","    update_init=False,\n",")"],"metadata":{"id":"b8L236Ba2ht5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756172653051,"user_tz":-120,"elapsed":50881,"user":{"displayName":"Yucheng Yang","userId":"06663760817085207108"}},"outputId":"1692a47b-a1ad-4f87-81bd-1f24c9532076"},"id":"b8L236Ba2ht5","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Average of total utility 20.068157.\n","The dataset has 4608 samples in total.\n"]}]},{"cell_type":"markdown","source":["## Step 2 — Initial value-function training (before any policy optimization)\n"],"metadata":{"id":"Xfm1boI6267z"},"id":"Xfm1boI6267z"},{"cell_type":"code","source":["%%time\n","vtrainers = []\n","for i in range(value_config[\"num_vnet\"]):\n","    config[\"vnet_idx\"] = str(i)\n","    vtrainers.append(ValueTrainer(config))\n","\n","for vtr in vtrainers:\n","    vtr.train(train_vds, valid_vds, value_config[\"num_epoch\"], value_config[\"batch_size\"])"],"metadata":{"id":"_xnf5tdG25DV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756172682431,"user_tz":-120,"elapsed":29378,"user":{"displayName":"Yucheng Yang","userId":"06663760817085207108"}},"outputId":"39d2b200-d48d-425c-af2f-80eb3f8e9b1a"},"id":"_xnf5tdG25DV","execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Value function learning epoch: 0\n","Value function learning epoch: 20\n","Value function learning epoch: 40\n","Value function learning epoch: 0\n","Value function learning epoch: 20\n","Value function learning epoch: 40\n","Value function learning epoch: 0\n","Value function learning epoch: 20\n","Value function learning epoch: 40\n","CPU times: user 30.5 s, sys: 2.12 s, total: 32.6 s\n","Wall time: 29.4 s\n"]}]},{"cell_type":"markdown","source":["> **Notes for readers:**\n",">\n","> * We pre-train the value network(s) once on data generated by an initial policy.\n","> * Each `ValueTrainer.train(...)` runs for `value_config[\"num_epoch\"]` epochs over the prepared datasets.\n","> * These $V$ nets will be used as the terminal bootstrap $\\beta^T V(s_T)$ inside policy optimization."],"metadata":{"id":"417DaJ_s3BRo"},"id":"417DaJ_s3BRo"},{"cell_type":"markdown","source":["\n","## Step 3 — Policy training with periodic value function updating\n","\n","**What happens here.**\n","\n","* We launch `KSPolicyTrainer.train(num_step, batch_size)`.\n","* **Every step**:\n","\n","  * draw a **fresh mini-batch** from `policy_ds` and **simulate new shocks** (`sampler`),\n","  * take **one policy gradient step** (`train_step`).\n","* **Policy-dataset refresh (from simulation)**:\n","\n","  * Inside `sampler`, when `policy_ds.epoch_used > epoch_resample`, we **rebuild the dataset** via `update_policydataset(update_init)`.\n","  * With your config `epoch_resample = 0`, this means: **rebuild after each full pass over the dataset** (cadence ≈ `ceil(dataset_rows / batch_size)` steps).\n","  * If `update_init=True` (set right after value retraining), the next rebuild is a **hard refresh**: we also update dataset stats from the new simulation.\n","* **Every `freq_valid` steps**: run validation on a fixed validation set.\n","* **Every `freq_update_v` steps** (if `value_sampling != \"bchmk\"`):\n","\n","  * **rebuild value datasets** under the **current policy**,\n","  * **retrain** each value net for `value_config[\"num_epoch\"]` epochs,\n","  * set `update_init=True` so the **next policy-dataset rebuild** performs a **hard refresh**.\n"],"metadata":{"id":"FV6j5Tzb3PL7"},"id":"FV6j5Tzb3PL7"},{"cell_type":"code","source":["# Iterative policy and value training\n","policy_config = config[\"policy_config\"]"],"metadata":{"id":"fDPAlwctCC33","executionInfo":{"status":"ok","timestamp":1756173087056,"user_tz":-120,"elapsed":12,"user":{"displayName":"Yucheng Yang","userId":"06663760817085207108"}}},"id":"fDPAlwctCC33","execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["\n","## Block A — PolicyTrainer (base class)\n","\n","> This is the **generic policy trainer** (abstract base class). It handles:\n",">\n","> * State preparation (`prepare_state`),\n","> * Policy evaluation (`policy_fn`),\n","> * Gradient calculation (`grad`),\n","> * Training loop (`train`).\n",">\n","> You **do not need to change this**. Read through it carefully.\n"],"metadata":{"id":"rDlWTMjIEsci"},"id":"rDlWTMjIEsci"},{"cell_type":"code","source":["from policy import PolicyTrainer"],"metadata":{"id":"artzNQQwCCme","executionInfo":{"status":"ok","timestamp":1756173208708,"user_tz":-120,"elapsed":1571,"user":{"displayName":"Yucheng Yang","userId":"06663760817085207108"}}},"id":"artzNQQwCCme","execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["## Block B — KSPolicyTrainer (student exercise)\n","\n","> Now implement the **objective**.\n",">\n","> Fill in the blanks:\n",">\n","> 1. **Policy output:** use `self.policy_fn(full_state_dict)[...,0]`.\n","> 2. **Game case:** apply `tf.stop_gradient` to fix others.\n","> 3. **Factor prices:** compute `R` and `wage` with the KS formulas.\n","> 4. **Budget constraint:** update `wealth`, `csmp`, and next-period `k_cross`.\n","> 5. **Utility accumulation:** add `β^t log(csmp)`.\n","> 6. **Terminal bootstrap:** average value net predictions at `t_unroll-1`.\n"],"metadata":{"id":"oYDlN_KHFXe6"},"id":"oYDlN_KHFXe6"},{"cell_type":"code","source":["class KSPolicyTrainer(PolicyTrainer):\n","    def __init__(self, vtrainers, init_ds, policy_path=None):\n","        super().__init__(vtrainers, init_ds, policy_path)\n","        if self.config[\"init_with_bchmk\"]:\n","            init_policy = self.init_ds.k_policy_bchmk\n","            policy_type = \"pde\"\n","        else:\n","            init_policy = self.init_ds.c_policy_const_share\n","            policy_type = \"nn_share\"\n","        self.policy_ds = self.init_ds.get_policydataset(init_policy, policy_type, update_init=False)\n","\n","    @tf.function\n","    def loss(self, input_data):\n","        k_cross = input_data[\"k_cross\"]\n","        ashock, ishock = input_data[\"ashock\"], input_data[\"ishock\"]\n","        util_sum = 0\n","\n","        for t in range(self.t_unroll):\n","            k_mean = tf.reduce_mean(k_cross, axis=1, keepdims=True)\n","            k_mean_tmp = tf.tile(k_mean, [1, self.mparam.n_agt])\n","            k_mean_tmp = tf.expand_dims(k_mean_tmp, axis=-1)\n","            i_tmp = ishock[:, :, t:t+1]\n","            a_tmp = tf.tile(ashock[:, t:t+1], [1, self.mparam.n_agt])\n","            a_tmp = tf.expand_dims(a_tmp, axis=2)\n","\n","            basic_s_tmp = tf.concat(\n","                [tf.expand_dims(k_cross, axis=-1), k_mean_tmp, a_tmp, i_tmp],\n","                axis=-1\n","            )\n","            basic_s_tmp = self.init_ds.normalize_data(basic_s_tmp, key=\"basic_s\", withtf=True)\n","            full_state_dict = {\n","                \"basic_s\": basic_s_tmp,\n","                \"agt_s\": self.init_ds.normalize_data(tf.expand_dims(k_cross, axis=-1), key=\"agt_s\", withtf=True)\n","            }\n","\n","            if t == self.t_unroll - 1:\n","                # --- terminal bootstrap ---\n","                value = 0\n","                for vtr in self.vtrainers:\n","                    value += self.init_ds.unnormalize_data(\n","                        vtr.value_fn(full_state_dict)[..., 0], key=\"value\", withtf=True\n","                    )\n","                value /= self.num_vnet\n","                util_sum += self.discount[t]*value\n","                continue\n","\n","            # (1) policy output\n","            c_share = ...   # TODO: call self.policy_fn(full_state_dict)[...,0]\n","\n","            # (2) game case\n","            if self.policy_config[\"opt_type\"] == \"game\":\n","                c_share = tf.concat(\n","                    [c_share[:, 0:1], tf.stop_gradient(c_share[:, 1:])],\n","                    axis=1\n","                )\n","\n","            # (3) prices\n","            tau = tf.where(ashock[:, t:t+1] < 1, self.mparam.tau_b, self.mparam.tau_g)\n","            emp = tf.where(\n","                ashock[:, t:t+1] < 1,\n","                self.mparam.l_bar*self.mparam.er_b,\n","                self.mparam.l_bar*self.mparam.er_g\n","            )\n","            tau, emp = tf.cast(tau, DTYPE), tf.cast(emp, DTYPE)\n","            R    = ...   # TODO: rental rate formula\n","            wage = ...   # TODO: wage formula\n","\n","            # (4) budget\n","            wealth = ...  # TODO: R*k_cross + (1-tau)*wage*l_bar*ishock + mu*wage*(1-ishock)\n","            csmp   = tf.clip_by_value(c_share * wealth, EPSILON, wealth-EPSILON)\n","            k_cross = wealth - csmp\n","\n","            # (5) utility\n","            util_sum += self.discount[t] * tf.math.log(csmp)\n","\n","        if self.policy_config[\"opt_type\"] == \"socialplanner\":\n","            output_dict = {\"m_util\": -tf.reduce_mean(util_sum), \"k_end\": tf.reduce_mean(k_cross)}\n","        elif self.policy_config[\"opt_type\"] == \"game\":\n","            output_dict = {\"m_util\": -tf.reduce_mean(util_sum[:, 0]), \"k_end\": tf.reduce_mean(k_cross)}\n","        return output_dict\n","\n","    def update_policydataset(self, update_init=False):\n","        self.policy_ds = self.init_ds.get_policydataset(self.current_c_policy, \"nn_share\", update_init)\n","\n","    def get_valuedataset(self, update_init=False):\n","        return self.init_ds.get_valuedataset(self.current_c_policy, \"nn_share\", update_init)\n","\n","    def current_c_policy(self, k_cross, ashock, ishock):\n","        k_mean = np.mean(k_cross, axis=1, keepdims=True)\n","        k_mean = np.repeat(k_mean, self.mparam.n_agt, axis=1)\n","        ashock = np.repeat(ashock, self.mparam.n_agt, axis=1)\n","        basic_s = np.stack([k_cross, k_mean, ashock, ishock], axis=-1)\n","        basic_s = self.init_ds.normalize_data(basic_s, key=\"basic_s\")\n","        basic_s = basic_s.astype(NP_DTYPE)\n","        full_state_dict = {\n","            \"basic_s\": basic_s,\n","            \"agt_s\": self.init_ds.normalize_data(k_cross[:, :, None], key=\"agt_s\")\n","        }\n","        c_share = self.policy_fn(full_state_dict)[..., 0]\n","        return c_share\n","\n","    def simul_shocks(self, n_sample, T, mparam, state_init):\n","        return KS.simul_shocks(n_sample, T, mparam, state_init)\n"],"metadata":{"id":"9_1KwqFcFJDt","executionInfo":{"status":"ok","timestamp":1756173248460,"user_tz":-120,"elapsed":86,"user":{"displayName":"Yucheng Yang","userId":"06663760817085207108"}}},"id":"9_1KwqFcFJDt","execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["## Block C — Training call\n","\n","> Once you fill the blanks, you can run the training just like before:"],"metadata":{"id":"arfxNJcOGjjl"},"id":"arfxNJcOGjjl"},{"cell_type":"code","execution_count":null,"id":"QBG7i6r4NCdl","metadata":{"id":"QBG7i6r4NCdl"},"outputs":[],"source":["ptrainer = KSPolicyTrainer(vtrainers, init_ds)\n","ptrainer.train(policy_config[\"num_step\"], policy_config[\"batch_size\"])"]},{"cell_type":"markdown","source":["\n","> **Notes for readers:**\n",">\n","> * **Mini-batch & shocks:** new **every step**.\n","> * **Policy-dataset cadence:** with `t_sample=200` and `t_skip=4`, each path contributes \\~50 time-slices;\n",">   dataset rows ≈ `n_path * 50` (minus NaN rows). Rebuild after each full pass:\n",">   `steps_per_dataset ≈ ceil(dataset_rows / batch_size)`.\n",">   Example: if `n_path=384`, rows ≈ `384*50=19,200` → `19,200/384=50` steps per rebuild.\n","> * **Validation:** every `freq_valid=500` steps (20 times for `num_step=10,000`).\n","> * **Value retrain:** every `freq_update_v=2000` steps (5 times total). This sets `update_init=True`; the **next** dataset rebuild then also updates dataset statistics from the new simulation (hard refresh).\n",">"],"metadata":{"id":"Z9JmD2634U19"},"id":"Z9JmD2634U19"},{"cell_type":"code","execution_count":null,"id":"IufSrjPnl775","metadata":{"id":"IufSrjPnl775"},"outputs":[],"source":["# Save config and models\n","with open(os.path.join(model_path, \"config.json\"), 'w') as f:\n","    json.dump(config, f)\n","\n","for i, vtr in enumerate(vtrainers):\n","    vtr.save_model(os.path.join(model_path, \"value{}.weights.h5\".format(i)))\n","\n","ptrainer.save_model(os.path.join(model_path, \"policy.weights.h5\"))\n","\n","end_time = time.monotonic()\n","print_elapsedtime(end_time - start_time)"]},{"cell_type":"code","execution_count":null,"id":"6OJ12AF432c2","metadata":{"id":"6OJ12AF432c2"},"outputs":[],"source":["model_path"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.11"}},"nbformat":4,"nbformat_minor":5}